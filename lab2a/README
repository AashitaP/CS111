NAME: Aashita Patwari
EMAIL: harshupatwari@gmail.com

Included files

C source modules:

lab2_add.c: Program for implementing shared variable add function. 
lab2_list.c: Program for implementing a sorted doubly linked circular list. 
SortedList.h: Header file for linked list operations.
SortedList.c: C module for linked list operations - insert, delete, lookup and length. 

Makefile with following targets:

build: compile all programs
tests: run over 200 test cases to generate the csv files. 
graphs: generate graphs using the csv files.
dist: create deliverable tarball
clean: delete compiled program and tarball (according to sanity test script)

csv files:

lab2_add.csv: results for Part 1 tests
lab2_list.csv: results for Part 2 tests

graphs:

lab2_add-1.png : threads and iterations required to generate a failure (with and without yields)
lab2_add-2.png : average time per operation with and without yields.
lab2_add-3.png : average time per (single threaded) operation vs. the number of iterations.
lab2_add-4.png : threads and iterations that can run successfully with yields under each of the synchronization options.
lab2_add-5.png : average time per (protected) operation vs. the number of threads.
lab2_list-1.png : average time per (single threaded) unprotected operation vs. number of iterations (illustrating the correction of the per-operation cost for the list length).
lab2_list-2.png : threads and iterations required to generate a failure (with and without yields).
lab2_list-3.png : iterations that can run (protected) without failure.
lab2_list-4.png : (length-adjusted) cost per operation vs the number of threads for the various synchronization options.




QUESTIONS

2.1.1 Why does it take many iterations before errors are seen?
Why does a significantly smaller number of iterations so seldom fail?

With fewer iterations, it is unlikely that the threads will try to access the critical section at the same time. With many iterations, it is possible that a thread did not finish adding and subtracting before it is preempted by another, resulting in a race condition and errors. 

2.1.2 
Why are the --yield runs so much slower? Where is the additional time going? Is it possible to get valid per operation timings - why or why not?

The sched_yield operation causes the thread to immediately yield instead of waiting for a time slice end. This results in additional overhead as more context switches have to be performed to save the current state and restore the other thread's state, resulting in slower time. No, the overhead forms the bulk of the time and hence, the results do not accurately show per-operation timings. Only with a very large number of iterations is it possible to get valid timings. 

2.1.3
Why does the average cost per op drop with increasing iterations? If cost per iteration is a function of the number of iterations, how do we know how many iterations to run?

Creating the thread takes up time so when the number of iterations increase, the cost associated with pthread_create is spread across the iterations resulting in an overall drop in average cost per operation. 

The solution to this would be to increase the number of iterations such that it is large enough so that the 'creating' cost is not significant and instead the cost per operation converges to a certain value. 

2.1.4
Why do all perform similarly for fewer threads? Why do three protected operations slow down as number of threads rises?

With fewer threads, the amount of time spent waiting is low so the overhead associated with having protected operations is not significant - resulting in similar performances. However as the number of threads rises, for synchronized execution, more threads will be forced to wait to enter the critical section resulting in more time wasted and slower performance.

2.2.1
Compare the variation in time per mutex protected operation vs number of threads in part 1 and 2. Comment on general shapes of curves & relative rates of increase and differences in shapes

Curves for both part 1 and part 2 increase linearly with the number of threads, with a general diagonal line. The slope for part 1 however reduces as the number of threads increase whereas for part 2, it remains relatively steep. 

The increase is likely due to an increase in overhead due to the synchronization locks, with increasing number of threads. Relative rate of increase for linked list is greater because linked lists do not scale relatively well due to the large number of operations and the complexity of the operations, and hence large critical section which results in alot of waiting time & overhead with more threads. Add operations on the other hand are simpler. 


2.2.2
Compare variation in time per protected operation vs number of threads for list operations protected by Mutex vs Spin locks. Comment on the general shapes of the curves and relative rates of increase & differences in shapes of the curves. 

Both curves increase linearly as the number of threads increase, but the slope and the rate of increase for spin locks is greater than for mutexes. This is because with greater contention, spin locks are more inefficient than mutexes as a thread that is unable to acquire the lock will simply spin and waste CPU cycles unlike mutexes, and thus cost increases at a greater rate. 




